import random

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchtext

from torchvision.utils import make_grid
from config import *
import numpy as np


def get_datapath(image_dim):
    if image_dim == 0:
        return ORG_DATASET_PATH_IMAGE0
    if image_dim == 1:
        return ORG_DATASET_PATH_IMAGE1
    if image_dim == 64:
        return ORG_DATASET_PATH_IMAGE64
    if image_dim == 128:
        return ORG_DATASET_PATH_IMAGE128
    if image_dim == 256:
        return ORG_DATASET_PATH_IMAGE256
    if image_dim == 512:
        return ORG_DATASET_PATH_IMAGE512
    if image_dim == 1024:
        return ORG_DATASET_PATH_IMAGE1024

    raise Exception("Unknown Image dim given")


def get_image_data_loaders(data_path=ORG_DATASET_PATH_IMAGE64, image_dim=64, train_split=0.8, batch_size=256):
    workers_count = min(int(CPU_COUNT * 0.80), batch_size)

    transform = torchvision.transforms.Compose([
        # transforms.RandomRotation(10),
        torchvision.transforms.Grayscale(num_output_channels=1),
        torchvision.transforms.Resize((image_dim, image_dim)),
        torchvision.transforms.ToTensor(),
    ])

    dataset = torchvision.datasets.ImageFolder(data_path, transform=transform)
    dataset_len = len(dataset)
    # dataset_len = 10000
    indices = list(range(dataset_len))

    random.shuffle(indices)
    split = int(np.floor(train_split * dataset_len))

    train_loader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size,
        sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),
        num_workers=workers_count)

    val_loader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size,
        sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:dataset_len]),
        num_workers=workers_count)

    train_set_len = len(train_loader) * batch_size
    val_set_len = len(val_loader) * batch_size
    class_names = dataset.classes
    num_of_classes = len(dataset.classes)

    return train_loader, val_loader, dataset_len, class_names


def get_opcode_data_loaders(batch_size=512):
    TEXT = torchtext.data.Field()
    LABEL = torchtext.data.LabelField()

    fields = {'text': ('text', TEXT), 'label': ('label', LABEL)}

    train_data, test_data = torchtext.data.TabularDataset.splits(
        path=os.path.join(os.getcwd()),
        train=ORG_DATASET_OPCODES_TRAIN_SPLIT_JSON,
        test=ORG_DATASET_OPCODES_TEST_SPLIT_JSON,
        format='json',
        fields=fields
    )
    TEXT.build_vocab(train_data)
    LABEL.build_vocab(train_data)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    train_iterator, test_iterator = torchtext.data.BucketIterator.splits((train_data, test_data),
                                                                         batch_size=batch_size,
                                                                         sort=False,
                                                                         shuffle=True,
                                                                         repeat=False,
                                                                         device=device)

    dataset_len = len(train_data) + len(test_data)
    class_names = list(LABEL.vocab.stoi.keys())
    text_vocal_len = len(TEXT.vocab)
    label_vocab_len = len(LABEL.vocab)

    pad_idx = TEXT.vocab.stoi[TEXT.pad_token]

    return train_iterator, test_iterator, dataset_len, class_names, text_vocal_len, label_vocab_len, pad_idx


def get_data_loaders(data_path=ORG_DATASET_PATH_IMAGE64, image_dim=64, train_split=0.8, batch_size=256,
                     data_type=DATA_TYPE_IMAGE):
    if data_type == DATA_TYPE_IMAGE:
        return get_image_data_loaders(data_path=data_path, image_dim=image_dim, train_split=train_split,
                                      batch_size=batch_size)
    if data_type == DATA_TYPE_OPCODE:
        return get_opcode_data_loaders(batch_size=batch_size)

    raise Exception("Unknown Datatype passed for Data Loaders")
